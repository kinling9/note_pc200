# 图形处理器：数据级并行

## GPU历史

### 统一处理器阵列

- 可扩展性好
- 资源利用率高
  - 流水线
- 适应图形API的新发展
- 实现通用计算
  - 计算映射

## GPU编程的发展

- GPGPU(General Purpose computation on GPU)
- 使用并行编程语言和API（CUDA、OpenCL等）
- 适用于大规模数据并行问题


> 任务 计算y = ax + y
### 串行
``` c++
void saxpy_serial(int n, float a, float *x, float *y) {
  for (int i = 0; i < n; ++i)
    y[i] = a * x[i] + y[i];
}
// Invoke serial SAXPY kernel
saxpy_serial(n, 2.0, x, y);

```
### CUDA
```C++
__global__
void saxpy_kernel(int n, float a, float *x, float *y) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  if(i < n) y[i] = a * x[i] + y[i];
}
// Invoke parallel SAXPY kernel (256 threads per block)
int nblocks = (n + 255) / 256;
saxpy_kernel<<<nblocks, 256>>>(n, 2.0, x, y);
```
#### kernel
- 由GPU执行的函数
- 细粒度：数据并行

#### 程序结构
```mermaid
graph LR
A[Grid] --> B[Block] --> C[Thread];
```
> 每个线程块不超过1024个线程

### OpenCL

``` c++
__kernel
void saxpy_kernel(int n, float a,
__global float *x,
__global float *y) {
  int i = get_global_id(0);
  if(i < n) y[i] = a*x[i] + y[i];
}
```

```mermaid
graph LR
A[NDRange] --> B[Work item] --> C[Workgroup];
```

## NVIDIA GPU计算结构
1. GPU SIMD 对于分支指令，不同的分支是交替执行的
2. 分支与汇合的行为是由编译器实现的，会被编译为的TX汇编代码

## NVIDIA GPU存储器结构
1. 大规模的并行需要极大的存储器带宽
2. 一般使用的内存结构包括GDDR和HBM
3. 存储器具有较多的共享级别，包括线程独有，块共用以及全局内存单元
### 存储器结构
1. SP包括大量的寄存器，会被多个线程均分，具有最高的执行效率
2. 局部存储器位于片外dram，从南方局部变量
3. 共享存储器包括在SM中，并由其上运行的线程所共用
4. 全局与常量存储器位于片外DRAM，实现GPU与主机之间的交互，由所有线程共享
5. cache结构 SM独立L1 Cache *程序员透明* 共享L2 Cache

## GPU CPU 对比
1. 控制器较为简单，分支类的命令执行效率很差 wrap 中不要出现分支
2. GPU的cache相对较少，需要通过多线程调度来隐藏时延
3. 相比CPU，GPU更容易达到由于存储限制限制的峰值性能

##  GPU体系结构的发展
1. 随着应用的变化，逐渐支持不同的数据类型
2. Fermi后的GPU架构实现了多Wrap和多发射的结构
3. 动态并行，即kernel可以独立的调用其他的kernel，实现动态的粒度划分
4. 实现统一的存储器访问，数据交换由CUDA系统进行数据的迁移，避免了手工的赋值过程
